{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"localization_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"kuR66aEenwBc","colab_type":"code","outputId":"000d130f-1524-43ec-e8c9-9a0710473bd1","executionInfo":{"status":"ok","timestamp":1555812189463,"user_tz":-330,"elapsed":1463,"user":{"displayName":"Anubhav Shrivastava","photoUrl":"","userId":"16211057153721648068"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"RXvuaLojJp9e","colab_type":"code","outputId":"b9844628-a82e-4e21-ce78-c16d4995df95","executionInfo":{"status":"ok","timestamp":1555812253460,"user_tz":-330,"elapsed":5042,"user":{"displayName":"Anubhav Shrivastava","photoUrl":"","userId":"16211057153721648068"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/My Drive/flipkart\")\n","# dir_content = os.listdir(\"test_set_64\")\n","# print(len(dir_content))\n","!ls"],"execution_count":10,"outputs":[{"output_type":"stream","text":[" cluster_images.ipynb\t\t     GeneratedDataset.pickle    test_set_64\n"," configuring_dataset.ipynb\t     localization_model.ipynb   train_64.csv\n","'Copy of localization_model.ipynb'   predictions.csv\t        training.csv\n"," deep_learning_localization.ipynb    test.csv\t\t        training_images\n"," GeneratedDataset128.pickle\t     testing_images\n"],"name":"stdout"}]},{"metadata":{"id":"42_2MmFTn_J0","colab_type":"code","outputId":"b54c9304-8edb-43aa-8ccf-5244386fbf12","executionInfo":{"status":"ok","timestamp":1550650238093,"user_tz":-330,"elapsed":598609,"user":{"displayName":"Anubhav Shrivastava","photoUrl":"","userId":"16211057153721648068"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import numpy as np\n","import os\n","import pandas as pd\n","import random\n","import tqdm\n","import cv2\n","os.chdir(\"/content/drive/My Drive/flipkart\")\n","# !ls\n","train_dir = os.listdir(\"train_set_64\")\n","random.shuffle(train_dir)\n","\n","train_sze = int(len(train_dir)*0.8);\n","X_train = train_dir[:train_sze]\n","X_test = train_dir[train_sze:]\n","\n","file = open(\"train_64.csv\",\"r\")\n","train_file = []\n","for line in file:\n","  train_file.append(line)\n","\n","y_train = []\n","for i in tqdm.tqdm(range(len(X_train))):\n","  for tmp in train_file:\n","    tmp = tmp.split(\",\")\n","    if(tmp[0] == X_train[i]):\n","      X_train[i] = cv2.imread(\"train_set_64/\"+tmp[0])\n","      tmp[1] = int(tmp[1])\n","      tmp[2] = int(tmp[2])\n","      tmp[3] = int(tmp[3])\n","      tmp[4] = int(tmp[4][:-1])\n","      y_train.append([tmp[1],tmp[2],tmp[3],tmp[4]])\n","      break\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 19200/19200 [09:57<00:00, 32.15it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"Ck9sOD4A-QIa","colab_type":"code","outputId":"181f5b2b-c007-42cd-97c4-06825293da3d","executionInfo":{"status":"ok","timestamp":1550651285361,"user_tz":-330,"elapsed":155590,"user":{"displayName":"Anubhav Shrivastava","photoUrl":"","userId":"16211057153721648068"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["y_test = []\n","for i in tqdm.tqdm(range(len(X_test))):\n","  for tmp in train_file:\n","    tmp = tmp.split(\",\")\n","    if(tmp[0] == X_test[i]):\n","      X_test[i] = cv2.imread(\"train_set_64/\"+tmp[0])\n","      tmp[1] = int(tmp[1])\n","      tmp[2] = int(tmp[2])\n","      tmp[3] = int(tmp[3])\n","      tmp[4] = int(tmp[4][:-1])\n","      y_test.append([tmp[1],tmp[2],tmp[3],tmp[4]])\n","      break\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 4800/4800 [02:34<00:00, 31.01it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"HsYjMLKvxQK0","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","import os\n","os.chdir(\"/content/drive/My Drive/flipkart\")\n","\n","# with open('GeneratedDataset.pickle','wb') as f:\n","# \tpickle.dump([X_train,y_train,X_test,y_test],f)\n","pickle_in = open('GeneratedDataset.pickle','rb')\n","[X_train,y_train,X_test,y_test] = pickle.load(pickle_in)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"79EL0jq6Qbb1","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","X_tr = np.array(X_train)\n","X_sum = np.sum(X_tr)\n","X_avg = (X_sum / X_tr.size)\n","X_sq = (X_train - X_avg)**2\n","X_sd = np.sqrt(np.sum(X_sq)/X_tr.size + 0.001)\n","\n","X_train = (X_train - X_avg)/X_sd\n","X_test = (X_test - X_avg)/X_sd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f9XgsCHizlVh","colab_type":"code","colab":{}},"cell_type":"code","source":["def find_accuracy(y_pred,y):\n","  cnt = 0\n","  print(y_pred.shape)\n","  l,_ = y_pred.shape\n","  for i in range(l):\n","    xA = [0,0,0,0]\n","    xB = [0,0,0,0]\n","    xA[0] = y_pred[i][0]-y_pred[i][2]/2\n","    xA[1] = y_pred[i][1]-y_pred[i][3]/2\n","    xA[2] = y_pred[i][0]+y_pred[i][2]/2\n","    xA[3] = y_pred[i][1]+y_pred[i][3]/2\n","    \n","    xA = list(map((lambda x: max(0,x)), xA))\n","    xA = list(map((lambda x: min(64,x)), xA))\n","\n","    xB[0] = y[i][0]-y[i][2]/2\n","    xB[1] = y[i][1]-y[i][3]/2\n","    xB[2] = y[i][0]+y[i][2]/2\n","    xB[3] = y[i][1]+y[i][3]/2\n","    \n","    xB = list(map((lambda x: max(0,x)), xB))\n","    xB = list(map((lambda x: min(64,x)), xB))\n","    \n","    cnt += float(bb_intersection_over_union(xA,xB))\n","\n","  cnt = cnt/len(y_pred)\n","  return cnt\n","\n","def bb_intersection_over_union(boxA, boxB):\n","  xA = max(boxA[0], boxB[0])\n","  yA = max(boxA[1], boxB[1])\n","  xB = min(boxA[2], boxB[2])\n","  yB = min(boxA[3], boxB[3])\n","\n","  interArea = max(0,(xB - xA)) * max(0,(yB - yA))\n","  boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n","  boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n","\n","  iou = interArea / float(boxAArea + boxBArea - interArea)\n","\n","  return iou\n","\n","def tf_iou(boxes1, boxes2):\n","    def run(tb1, tb2):\n","        x11, y11, x12, y12 = tf.split(tb1, 4, axis=1)\n","        x21, y21, x22, y22 = tf.split(tb2, 4, axis=1)\n","\n","        xA = tf.maximum(x11, tf.transpose(x21))\n","        yA = tf.maximum(y11, tf.transpose(y21))\n","        xB = tf.minimum(x12, tf.transpose(x22))\n","        yB = tf.minimum(y12, tf.transpose(y22))\n","\n","        interArea = tf.maximum((xB - xA + 1), 0) * tf.maximum((yB - yA + 1), 0)\n","\n","        boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n","        boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n","\n","        iou = interArea / (boxAArea + tf.transpose(boxBArea) - interArea)\n","\n","        return iou\n","\n","\n","def scaled_loss(y, y_hat):\n","    \n","    x_tmp = (y[:,0])\n","    y_tmp = (y[:,1])\n","    h_tmp = (y[:,2])\n","    w_tmp = (y[:,3])\n","    x1 = x_tmp-h_tmp/2;\n","    x2 = x_tmp+h_tmp/2;\n","    y1 = y_tmp-w_tmp/2;\n","    y2 = y_tmp+w_tmp/2;\n","    \n","    x_hat = (y_hat[:, 0])\n","    y_hat = (y_hat[:, 1])\n","    h_hat = (y_hat[:, 2])\n","    w_hat = (y_hat[:, 3])\n","    x1_hat = tf.subtract(x_hat,tf.divide(h_hat,2));\n","    x2_hat = tf.add(x_hat,tf.divide(h_hat,2));\n","    y1_hat = tf.subtract(y_hat,tf.divide(w_hat,2));\n","    y2_hat = tf.add(y_hat,tf.divide(w_hat,2));\n","    \n","#     x1_hat = y_hat[:, 0]\n","#     x2_hat = y_hat[:, 1]\n","#     y1_hat = y_hat[:, 2]\n","#     y2_hat = y_hat[:, 3]\n","    ix1 = tf.maximum(x1_hat, x1)\n","    ix2 = tf.minimum(x2_hat, x2)\n","    iy1 = tf.maximum(y1_hat, y1)\n","    iy2 = tf.minimum(y2_hat, y2)\n","    tensor_type = x1.dtype\n","    y_hat_area = tf.multiply(tf.maximum(tf.cast(0.0, tensor_type), x2_hat - x1_hat),\n","                             tf.maximum(tf.cast(0.0, tensor_type), y2_hat - y1_hat))\n","    y_area = tf.multiply(tf.maximum(tf.cast(0.0, tensor_type), x2 - x1),\n","                         tf.maximum(tf.cast(0.0, tensor_type), y2 - y1))\n","    i_area = tf.multiply(tf.maximum(tf.cast(0.0, tensor_type), ix2 - ix1),\n","                         tf.maximum(tf.cast(0.0, tensor_type), iy2 - iy1))\n","    u_area = y_hat_area + y_area - i_area\n","    uia = (u_area - i_area) / (y_area + 1e-8)\n","    return tf.reduce_mean(uia)\n","  \n","def mIOU(y, y_hat):\n","    x1_hat = y_hat[:, 0]\n","    x2_hat = y_hat[:, 1]\n","    y1_hat = y_hat[:, 2]\n","    y2_hat = y_hat[:, 3]\n","    x1 = y[:, 0]\n","    x2 = y[:, 1]\n","    y1 = y[:, 2]\n","    y2 = y[:, 3]\n","    ix1 = tf.maximum(x1_hat, x1)\n","    ix2 = tf.minimum(x2_hat, x2)\n","    iy1 = tf.maximum(y1_hat, y1)\n","    iy2 = tf.minimum(y2_hat, y2)\n","    tensor_type = x1.dtype\n","    y_hat_area = tf.multiply(tf.maximum(tf.cast(0.0, tensor_type), x2_hat - x1_hat),\n","                             tf.maximum(tf.cast(0.0, tensor_type), y2_hat - y1_hat))\n","    y_area = tf.multiply(tf.maximum(tf.cast(0.0, tensor_type), x2 - x1),\n","                         tf.maximum(tf.cast(0.0, tensor_type), y2 - y1))\n","    i_area = tf.multiply(tf.maximum(tf.cast(0.0, tensor_type), ix2 - ix1),\n","                         tf.maximum(tf.cast(0.0, tensor_type), iy2 - iy1))\n","    u_area = y_hat_area + y_area - i_area\n","    iou = i_area / u_area\n","    return tf.reduce_mean(iou)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F-ayrdfcoQQ2","colab_type":"code","outputId":"70c3ec34-0d8a-4ee8-faef-146dcf5f9d27","executionInfo":{"status":"ok","timestamp":1555813454636,"user_tz":-330,"elapsed":1146255,"user":{"displayName":"Anubhav Shrivastava","photoUrl":"","userId":"16211057153721648068"}},"colab":{"base_uri":"https://localhost:8080/","height":1108}},"cell_type":"code","source":["# 96.05% for 5 epochs\n","# 99.21% for 10 epochs with xavier initializer\n","import tensorflow as tf\n","import sys\n","import numpy as np\n","\n","n_classes = 4\n","batch_size = 128\n","\n","X = tf.placeholder(tf.float32,[None,64,64,3])\n","y = tf.placeholder(tf.float32)\n","\n","is_train = tf.placeholder(tf.bool, name=\"is_train\");\n","\n","initializer = tf.contrib.layers.xavier_initializer()\n","\n","layer = {'conv1_weights':tf.Variable(initializer([3,3,3,64])),\n","          'conv2_weights':tf.Variable(initializer([3,3,64,64])),\n","          'conv3_weights':tf.Variable(initializer([3,3,64,128])),\n","          'conv4_weights':tf.Variable(initializer([3,3,128,128])),\n","          'conv5_weights':tf.Variable(initializer([3,3,128,256])),\n","          'conv6_weights':tf.Variable(initializer([3,3,256,256])),\n","          'conv7_weights':tf.Variable(initializer([3,3,256,512])),\n","          'conv8_weights':tf.Variable(initializer([3,3,512,512])),\n","          'conv9_weights':tf.Variable(initializer([3,3,512,512])),\n","          'fc1_weights':tf.Variable(initializer([4*4*512,4096])),\n","          'fc2_weights':tf.Variable(initializer([4096,4096])),\n","          'fc3_weights':tf.Variable(initializer([4096,1024])),\n","         'out_weights':tf.Variable(initializer([1024,n_classes]))}\n","biases = {'conv1_biases':tf.Variable(initializer([64])),\n","          'conv2_biases':tf.Variable(initializer([64])),\n","          'conv3_biases':tf.Variable(initializer([128])),\n","          'conv4_biases':tf.Variable(initializer([128])),\n","          'conv5_biases':tf.Variable(initializer([256])),\n","          'conv6_biases':tf.Variable(initializer([256])),\n","          'conv7_biases':tf.Variable(initializer([512])),\n","          'conv8_biases':tf.Variable(initializer([512])),\n","          'conv9_biases':tf.Variable(initializer([512])),\n","          'fc1_biases':tf.Variable(initializer([4096])),\n","          'fc2_biases':tf.Variable(initializer([4096])),\n","          'fc3_biases':tf.Variable(initializer([1024])),\n","          'out_biases':tf.Variable(initializer([n_classes]))}\n","\n","keep_prob = tf.placeholder(tf.float32)\n","\n","def cnn_model(X):\n","  X = tf.reshape(X,[-1,64,64,3])\n","  \n","  conv1 = tf.nn.conv2d(X,layer['conv1_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv1_biases']\n","#   conv1 = tf.layers.batch_normalization(conv1, training=is_train)\n","  conv1 = tf.nn.relu(conv1)\n","#   conv1 = tf.nn.max_pool(conv1,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","\n","  conv2 = tf.nn.conv2d(conv1,layer['conv2_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv2_biases']\n","#   conv2 = tf.layers.batch_normalization(conv2, training=is_train)\n","  conv2 = tf.nn.relu(conv2)\n","  conv2 = tf.nn.max_pool(conv2,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","\n","  conv3 = tf.nn.conv2d(conv2,layer['conv3_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv3_biases']\n","#   conv3 = tf.layers.batch_normalization(conv3, training=is_train)\n","  conv3 = tf.nn.relu(conv3)\n","#   conv3 = tf.nn.max_pool(conv3,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","  \n","  conv4 = tf.nn.conv2d(conv3,layer['conv4_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv4_biases']\n","#   conv4 = tf.layers.batch_normalization(conv4, training=is_train)\n","  conv4 = tf.nn.relu(conv4)\n","  conv4 = tf.nn.max_pool(conv4,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","  \n","  conv5 = tf.nn.conv2d(conv4,layer['conv5_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv5_biases']\n","#   conv5 = tf.layers.batch_normalization(conv5, training=is_train)\n","  conv5 = tf.nn.relu(conv5)\n","#   conv5 = tf.nn.max_pool(conv5,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","  \n","  conv6 = tf.nn.conv2d(conv5,layer['conv6_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv6_biases']\n","#   conv6 = tf.layers.batch_normalization(conv6, training=is_train)\n","  conv6 = tf.nn.relu(conv6)\n","  conv6 = tf.nn.max_pool(conv6,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","  \n","  conv7 = tf.nn.conv2d(conv6,layer['conv7_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv7_biases']\n","#   conv7 = tf.layers.batch_normalization(conv7, training=is_train)\n","  conv7 = tf.nn.relu(conv7)\n","#   conv7 = tf.nn.max_pool(conv7,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","  \n","  conv8 = tf.nn.conv2d(conv7,layer['conv8_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv8_biases']\n","#   conv8 = tf.layers.batch_normalization(conv8, training=is_train)\n","  conv8 = tf.nn.relu(conv8)\n","#   conv8 = tf.nn.max_pool(conv8,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","  \n","  conv9 = tf.nn.conv2d(conv8,layer['conv9_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['conv9_biases']\n","#   conv9 = tf.layers.batch_normalization(conv9, training=is_train)\n","  conv9 = tf.nn.relu(conv9)\n","  conv9 = tf.nn.max_pool(conv9,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n","\n","#   fc = tf.nn.conv2d(conv9,layer['fc1_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['fc1_biases']\n","#   fc = tf.nn.relu(fc)\n","#   fc = tf.nn.dropout(fc,keep_prob)\n","  \n","#   fc = tf.nn.conv2d(fc,layer['fc2_weights'],strides = [1,1,1,1],padding = 'SAME') + biases['fc2_biases']\n","#   fc = tf.nn.relu(fc)\n","#   fc = tf.nn.dropout(fc,keep_prob)\n","  \n","  conv10 = tf.reshape(conv9,[-1,4*4*512])\n","  fc = tf.matmul(conv10,layer['fc1_weights']) + biases['fc1_biases']\n","#   fc = tf.layers.batch_normalization(fc, training=is_train)\n","  fc = tf.nn.relu(fc)\n","  fc = tf.nn.dropout(fc,keep_prob)\n","  \n","  fc = tf.matmul(fc,layer['fc2_weights']) + biases['fc2_biases']\n","#   fc = tf.layers.batch_normalization(fc, training=is_train)\n","  fc = tf.nn.relu(fc)\n","  fc = tf.nn.dropout(fc,keep_prob)\n","  \n","  fc = tf.matmul(fc,layer['fc3_weights']) + biases['fc3_biases']\n","#   fc = tf.layers.batch_normalization(fc, training=is_train)\n","  fc = tf.nn.relu(fc)\n","  fc = tf.nn.dropout(fc,keep_prob)\n","\n","  classification = tf.matmul(fc,layer['out_weights']) + biases['out_biases']\n","  \n","  return classification\n","\n","y_predict = cnn_model(X)\n","\n","# y_pred = np.array(y_predict)\n","# y_ans = np.array(y)\n","# cost = tf.reduce_mean(tf.reduce_sum(tf.abs(tf.subtract(y_predict, y)),axis = 1))\n","\n","cost = tf.reduce_mean(tf.nn.l2_loss(tf.subtract(y_predict,y)))\n","# cost = scaled_loss(y_predict,y)\n","\n","# update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","# with tf.control_dependencies(update_ops):\n","model = tf.train.AdamOptimizer(0.0001).minimize(cost)\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","\n","# y1 = sess.run(y_predict,feed_dict = {X:X_train[:5],y:y_train[:5],keep_prob:1.0})\n","# y2 = sess.run(y,feed_dict = {X:X_train[:5],y:y_train[:5],keep_prob:1.0})\n","# # y1 = np.array(y1)\n","# # y2 = np.array(y2)\n","# print(tf.reduce_sum(tf.math.abs(y1-y2),axis = 1))\n","\n","num_iter = 50\n","\n","for i in range(num_iter):\n","    iter_loss=0\n","    ind = 0\n","    for j in range(int(len(X_train)/128)):\n","        batch_x = X_train[ind:ind+128]\n","        batch_y = y_train[ind:ind+128]\n","        ind+=128\n","        _,c=sess.run([model,cost],feed_dict = {X:batch_x,y:batch_y,keep_prob:0.5,is_train:True})\n","        iter_loss += c\n","    print(i,iter_loss)\n","    \n","\n","# accuracy = accuracy(y_predict,y)\n","# print(sess.run(accuracy,feed_dict = {X:X_test,y:y_test,keep_prob:1.0}))\n","# sess.close()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-16-5cfaf4dcbc85>:104: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","0 7082869.162109375\n","1 3397997.3359375\n","2 1937950.7060546875\n","3 1666180.9008789062\n","4 1546924.6420898438\n","5 1413217.4560546875\n","6 1321551.05078125\n","7 1253567.2172851562\n","8 1221114.033203125\n","9 1158621.1342773438\n","10 1094923.38671875\n","11 1064427.0080566406\n","12 1005679.0913085938\n","13 935316.4487304688\n","14 919653.005859375\n","15 882098.5729980469\n","16 826858.1657714844\n","17 768681.130859375\n","18 740025.9560546875\n","19 683395.2141113281\n","20 662412.5615234375\n","21 646981.7141113281\n","22 649122.3696289062\n","23 642719.3801269531\n","24 729627.169921875\n","25 634568.4885253906\n","26 603494.3767089844\n","27 685273.7573242188\n","28 702810.7856445312\n","29 532764.86328125\n","30 589290.8928222656\n","31 643417.2690429688\n","32 647513.2150878906\n","33 607173.4841308594\n","34 591843.2534179688\n","35 509751.8039550781\n","36 453461.52099609375\n","37 514215.08544921875\n","38 504328.5642089844\n","39 575443.01953125\n","40 608239.3842773438\n","41 525770.5893554688\n","42 508871.00048828125\n","43 491716.0087890625\n","44 548237.4904785156\n","45 535526.2084960938\n","46 776989.46484375\n","47 630674.3933105469\n","48 560642.6560058594\n","49 534034.7478027344\n"],"name":"stdout"}]},{"metadata":{"id":"37hiD05f2iew","colab_type":"code","outputId":"7931915b-3cc5-4794-c78b-a6773001b1fd","executionInfo":{"status":"error","timestamp":1555814446069,"user_tz":-330,"elapsed":1570,"user":{"displayName":"Anubhav Shrivastava","photoUrl":"","userId":"16211057153721648068"}},"colab":{"base_uri":"https://localhost:8080/","height":792}},"cell_type":"code","source":["ans_y = sess.run(y_predict,feed_dict = {X:X_test[:500],keep_prob:1})\n","actual_y = sess.run(y,feed_dict = {y:y_test[:500],keep_prob:1})\n","accuracy = find_accuracy(np.array(ans_y),np.array(actual_y))\n","print(accuracy)"],"execution_count":28,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1092\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3566\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3567\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Can not convert a int into a Tensor.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-df9a7cc93928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mans_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mactual_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1095\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Can not convert a int into a Tensor."]}]},{"metadata":{"id":"JBYTNFYTP0ZP","colab_type":"code","outputId":"dca03cd5-f6a2-49ae-d2e7-21f18fc1efcc","executionInfo":{"status":"ok","timestamp":1555814258894,"user_tz":-330,"elapsed":10845,"user":{"displayName":"Anubhav Shrivastava","photoUrl":"","userId":"16211057153721648068"}},"colab":{"base_uri":"https://localhost:8080/","height":5397,"output_embedded_package_id":"1a1FilOZBz1CELRZs8lKb7IY7KheTfewu"}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import cv2\n","import copy\n","import os\n","import numpy as np\n","import tqdm\n","\n","testing_dir = os.listdir(\"testing_images\")\n","\n","for img_name in testing_dir[20:40]:\n","  image = cv2.imread(\"testing_images/\" + img_name)\n","  image2 = cv2.resize(image,(64,64))\n","  img2 = np.reshape(image2,(-1,64,64,3))\n","  c=sess.run(y_predict,feed_dict = {X:img2,keep_prob:1})\n","  img = image\n","  x = int(round(c[0][0]))\n","  y = int(round(c[0][1]))\n","  h = int(round(c[0][2]))\n","  w = int(round(c[0][3]))\n","  x1 = x-int(h/2);\n","  x2 = x+int(h/2);\n","  y1 = y-int(w/2);\n","  y2 = y+int(w/2);\n","  [x1,y1,x2,y2] = list(map((lambda x: max(0,x)),[x1,y1,x2,y2]))\n","  [x1,y1,x2,y2] = list(map((lambda x: min(63,x)),[x1,y1,x2,y2]))\n","  x1 = int(round((x1*480)/64))\n","  x2 = int(round((x2*480)/64))\n","  y1 = int(round((y1*640)/64))\n","  y2 = int(round((y2*640)/64))\n","  \n","  print(x1,y1,x2,y2)\n","  for i in range(x1,x2):\n","    img[i][y1][0] = 0;\n","    img[i][y1][0] = 128;\n","    img[i][y1][0] = 0;\n","    img[i][y1+1][0] = 0;\n","    img[i][y1+1][0] = 128;\n","    img[i][y1+1][0] = 0;\n","  for i in range(y1,y2):\n","    img[x1][i][0] = 0;\n","    img[x1][i][0] = 128;\n","    img[x1][i][0] = 0;\n","  for i in range(y1,y2):\n","    img[x2-1][i][0] = 0;\n","    img[x2-1][i][0] = 128;\n","    img[x2-1][i][0] = 0;\n","  for i in range(x1,x2):\n","    img[i][y2-1][0] = 0;\n","    img[i][y2-1][0] = 128;\n","    img[i][y2-1][0] = 0;\n","  image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","  plt.figure()\n","  plt.imshow(image)\n"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"metadata":{"id":"vB6ahOKbxbcZ","colab_type":"code","outputId":"687742ec-e8b5-4c22-8570-15f863718f13","executionInfo":{"status":"ok","timestamp":1549602324120,"user_tz":-330,"elapsed":1381950,"user":{"displayName":"Anubhav Shrivastava","photoUrl":"","userId":"16211057153721648068"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[{"output_type":"stream","text":[" configuring_dataset.ipynb\t     localization_model.ipynb   train.csv\n","'Copy of localization_model.ipynb'   predictions.csv\t        training.csv\n"," deep_learning_localization.ipynb    test.csv\t\t        training_images\n"," GeneratedDataset.pickle\t     testing_images\t        train_set\n"],"name":"stdout"}]}]}